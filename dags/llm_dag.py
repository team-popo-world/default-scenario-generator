from datetime import datetime, timedelta
from airflow import DAG
import pandas as pd
import os
import logging
from pymongo import MongoClient
from report_llm.utils.load_db import load_userId, load_data
from report_llm.utils.llm import get_llm_chain
from report_llm.main import generate_and_update
# Airflow ë²„ì „ í˜¸í™˜ì„±ì„ ìœ„í•œ PythonOperator import
try:
    # Airflow 3.0+ ë²„ì „ìš© (í‘œì¤€ í”„ë¡œë°”ì´ë”)
    from airflow.providers.standard.operators.python import PythonOperator
except ImportError:
    try:
        # Airflow 2.8+ ë²„ì „ìš© (ê¸°ì¡´ ê²½ë¡œ)
        from airflow.operators.python import PythonOperator
    except ImportError:
        # Airflow 2.7 ì´í•˜ ë²„ì „ìš©
        from airflow.operators.python_operator import PythonOperator

# ê¸°ë³¸ DAG ì„¤ì •
default_args = {
    'owner': 'llm-team',
    'depends_on_past': False,
    'start_date': datetime(2025, 6, 13),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'weekly_analysis_update_pipeline', # DAG ì´ë¦„ (Airflow UIì— í‘œì‹œ)
    default_args=default_args,
    description='ì•„ì´ ë ˆí¬íŠ¸ ìƒì„± ë° ì—…ë°ì´íŠ¸ íŒŒì´í”„ë¼ì¸', # ì„¤ëª… (UIìš©)
    schedule_interval='0 0 * * 1',  # ì‹¤í–‰ ì£¼ê¸°: ë§¤ì£¼ ì›”ìš”ì¼ ìì • (cron í˜•ì‹)
    catchup=False, # ê³¼ê±° ëˆ„ë½ëœ ì‹¤í–‰ ì—¬ë¶€ ë¬´ì‹œ (Falseë©´ í˜„ì¬ ì‹œì ë¶€í„° ì‹¤í–‰)
    tags=['api', 'generation_update'], # UIì—ì„œ DAG íƒœê·¸ë¡œ í•„í„°ë§ ê°€ëŠ¥
)

# api ë¶ˆëŸ¬ì˜¤ê¸° -> llmìœ¼ë¡œ report ìƒì„± -> DBì— ì—…ë°ì´íŠ¸
def call_api(ti):
    logging.info("ğŸ”¹ call_api ì‹œì‘")

    user_list = load_userId()
    logging.info(f"âœ… user_list ë¡œë“œ ì™„ë£Œ: {len(user_list)}ëª…")
    
    invest_merged_df, quest_merged_df, shop_merged_df, cluster_df = load_data(user_list)
    logging.info("âœ… ë°ì´í„° ë³‘í•© ì™„ë£Œ")

    # XComìœ¼ë¡œ í‘¸ì‹œ
    ti.xcom_push(key="user_list", value=user_list)
    ti.xcom_push(key="invest_merged_df", value=invest_merged_df.to_json())
    ti.xcom_push(key="quest_merged_df", value=quest_merged_df.to_json())
    ti.xcom_push(key="shop_merged_df", value=shop_merged_df.to_json())
    ti.xcom_push(key="cluster_df", value=cluster_df.to_json())
    logging.info("ğŸ“¤ XCom push ì™„ë£Œ")

def generate_analysis_and_update(ti):
    logging.info("ğŸ”¹ generate_analysis_and_update ì‹œì‘")

    # XComì—ì„œ ë°ì´í„° ë¡œë“œ
    user_list = ti.xcom_pull(key="user_list", task_ids='call_api')
    logging.info(f"âœ… XComì—ì„œ user_list ë¡œë“œ: {len(user_list)}ëª…")

    invest_merged_df = pd.read_json(ti.xcom_pull(key="invest_merged_df", task_ids='call_api'))
    quest_merged_df = pd.read_json(ti.xcom_pull(key="quest_merged_df", task_ids='call_api'))
    shop_merged_df = pd.read_json(ti.xcom_pull(key="shop_merged_df", task_ids='call_api'))
    cluster_df = pd.read_json(ti.xcom_pull(key="cluster_df", task_ids='call_api'))
    logging.info("âœ… XComì—ì„œ ëª¨ë“  ë°ì´í„° ë¡œë“œ ì™„ë£Œ")

    chain = get_llm_chain()
    logging.info("âœ… LLM chain ì¤€ë¹„ ì™„ë£Œ")

    uri = os.getenv("MONGO_URI")
    db_name = os.getenv("MONGO_DB_NAME")
    client = MongoClient(uri) 
    db = client[db_name]
    user_collection = db["user_analysis"]
    graph_collection = db["user_graph"]

    for userId in user_list:
        logging.info(f"ğŸ§  ë¶„ì„ ì‹œì‘: {userId}")
        try:
            generate_and_update(userId, chain, invest_merged_df, cluster_df, quest_merged_df, shop_merged_df, user_collection, graph_collection)
            logging.info(f"âœ… ë¶„ì„ ë° ì €ì¥ ì™„ë£Œ: {userId}")
        except Exception as e:
            logging.error(f"âŒ {userId} ë¶„ì„ ì‹¤íŒ¨: {e}")


# Task ì •ì˜
call_api_task = PythonOperator(
    task_id='call_api',
    python_callable=call_api,
    dag=dag,
    execution_timeout=timedelta(minutes=30),
)

generate_and_update_task = PythonOperator(
    task_id='generate_analysis_and_update',
    python_callable=generate_analysis_and_update,
    dag=dag,
)

# Task ì˜ì¡´ì„± ì„¤ì •
# api ë¶ˆëŸ¬ì˜¤ê¸° -> llmìœ¼ë¡œ report ìƒì„± & DBì— ì—…ë°ì´íŠ¸
call_api_task >> generate_and_update_task