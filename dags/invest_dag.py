from datetime import datetime, timedelta
from airflow import DAG
import os
from sqlalchemy import create_engine, text
from dotenv import load_dotenv
import pandas as pd

# Airflow ë²„ì „ í˜¸í™˜ì„±ì„ ìœ„í•œ PythonOperator import
try:
    # Airflow 3.0+ ë²„ì „ìš© (í‘œì¤€ í”„ë¡œë°”ì´ë”)
    from airflow.providers.standard.operators.python import PythonOperator
except ImportError:
    try:
        # Airflow 2.8+ ë²„ì „ìš© (ê¸°ì¡´ ê²½ë¡œ)
        from airflow.operators.python import PythonOperator
    except ImportError:
        # Airflow 2.7 ì´í•˜ ë²„ì „ìš©
        from airflow.operators.python_operator import PythonOperator

from invest.main_preprocess import model_preprocess # ì „ì²˜ë¦¬ ëª¨ë¸ë§
from invest.main_train import model_train  # ëª¨ë¸ë§.py import í•˜ê¸°
from invest.main_updateDB import update_mongo_data # ë¶„ë¥˜ ê²°ê³¼ ëª½ê³ dbì— update
from invest.utils.data_utils import sanitize_dataframe_for_json, safe_dataframe_to_json

import subprocess
import time
import shutil 
import requests
import psutil


load_dotenv(override=True)

# ê¸°ë³¸ DAG ì„¤ì •
default_args = {
    'owner': 'invest-team',
    'depends_on_past': False,
    'start_date': datetime(2025, 6, 13),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'invest_cluster_pipeline',
    default_args=default_args,
    description='ì „ì²˜ë¦¬ ë° ëª¨ë¸ë§, ì „ì†¡ íŒŒì´í”„ë¼ì¸',
    schedule_interval='0 3 * * *',  # ë§¤ì¼ ì˜¤ì „ 3ì‹œ ì‹¤í–‰
    catchup=False,
    tags=['preprocessing', 'modeling', 'update'],
)


def start_mlflow_server():
    """MLflow ì„œë²„ë¥¼ S3 + RDSì™€ í•¨ê»˜ ì‹œì‘"""
    import requests
    import shutil
    
    # 1. ì„œë²„ê°€ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
    try:
        response = requests.get("http://43.203.175.69:5001/health", timeout=5)
        if response.status_code == 200:
            print("âœ… MLflow ì„œë²„ê°€ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.")
            return
    except:
        print("MLflow ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")

    # 2. MLflow ê²½ë¡œ ì°¾ê¸°
    mlflow_path = shutil.which("mlflow")
    if not mlflow_path:
        mlflow_path = "/home/ubuntu/mlflow_env/bin/mlflow"
        if not os.path.exists(mlflow_path):
            print(f"âŒ MLflowë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

    print(f"MLflow ê²½ë¡œ: {mlflow_path}")

    # 3. ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤ ì •ë¦¬
    os.system("sudo pkill -f 'mlflow server' 2>/dev/null")
    time.sleep(2)

    # 4. í™˜ê²½ë³€ìˆ˜ ì„¤ì • (AWS ìê²© ì¦ëª… í¬í•¨)
    env = os.environ.copy()
    env['AWS_ACCESS_KEY_ID'] = os.getenv('AWS_ACCESS_KEY_ID', '')
    env['AWS_SECRET_ACCESS_KEY'] = os.getenv('AWS_SECRET_ACCESS_KEY', '')
    env['AWS_DEFAULT_REGION'] = os.getenv('AWS_DEFAULT_REGION', 'ap-northeast-2')
    env['GIT_PYTHON_REFRESH'] = 'quiet'
    
    # AWS ìê²© ì¦ëª… í™•ì¸
    if not env['AWS_ACCESS_KEY_ID'] or not env['AWS_SECRET_ACCESS_KEY']:
        print("âš ï¸ AWS ìê²© ì¦ëª…ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("í™˜ê²½ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY")

    # 5. MLflow ì„œë²„ ì‹œì‘ (S3 + RDS ì™„ì „ í™œìš©)
    try:
        subprocess.Popen([
            mlflow_path, 'server',
            '--host', '0.0.0.0',
            '--port', '5001',
            # RDS PostgreSQL: ë©”íƒ€ë°ì´í„° ì €ì¥ (ì‹¤í—˜, ì‹¤í–‰, íŒŒë¼ë¯¸í„°, ë©”íŠ¸ë¦­)
            '--backend-store-uri', 
            'postgresql://postgres:team2%21123@mlflowdb-1.c3gseooicuve.ap-northeast-2.rds.amazonaws.com:5432/mlflowsercer_db',
            # S3 ë²„í‚·: ì•„í‹°íŒ©íŠ¸ ì €ì¥ (ëª¨ë¸, ì‹œê°í™”, íŒŒì¼)
            '--default-artifact-root', 's3://team2-mlflow-bucket',
            '--serve-artifacts'  # ì•„í‹°íŒ©íŠ¸ í”„ë¡ì‹œ í™œì„±í™”
        ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, env=env)

        # 6. ì„œë²„ ì‹œì‘ ëŒ€ê¸° (S3 ì—°ê²°ì„ ìœ„í•´ ë” ê¸´ ëŒ€ê¸°)
        time.sleep(25)

        # 7. ì„œë²„ í™•ì¸
        try:
            response = requests.get("http://localhost:5001/health", timeout=10)
            if response.status_code == 200:
                print("âœ… MLflow ì„œë²„ê°€ S3 + RDSì™€ í•¨ê»˜ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤!")
                print("ğŸ—„ï¸  ë©”íƒ€ë°ì´í„° ì €ì¥ì†Œ: PostgreSQL RDS (mlflowsercer_db)")
                print("ğŸ“¦ ì•„í‹°íŒ©íŠ¸ ì €ì¥ì†Œ: S3 (team2-mlflow-bucket)")
                print("ğŸŒ MLflow UI: http://43.203.175.69:5001")
            else:
                print("âš ï¸ MLflow ì„œë²„ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨")
        except Exception as e:
            print(f"âš ï¸ MLflow ì„œë²„ ì ‘ê·¼ ì‹¤íŒ¨: {e}")

    except Exception as e:
        print(f"âŒ MLflow ì„œë²„ ì‹œì‘ ì‹¤íŒ¨: {e}")




def preprocess_data(**context):
    try:
        print("ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...")
        df = model_preprocess()
        
        if df is None:
            raise ValueError("ì „ì²˜ë¦¬ í•¨ìˆ˜ê°€ Noneì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤")
        
        if df.empty:
            raise ValueError("ì „ì²˜ë¦¬ ê²°ê³¼ DataFrameì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
            
        # ë°ì´í„° í’ˆì§ˆ ê²€ì¦
        if df.isnull().all().all():
            raise ValueError("ì „ì²˜ë¦¬ ê²°ê³¼ê°€ ëª¨ë‘ null ê°’ì…ë‹ˆë‹¤")
            
        print(f"ì „ì²˜ë¦¬ ì™„ë£Œ - í–‰ ìˆ˜: {len(df)}, ì—´ ìˆ˜: {len(df.columns)}")
        
        # **í•µì‹¬: DataFrame ì •ë¦¬ ë° ì•ˆì „í•œ ë³€í™˜**
        df = sanitize_dataframe_for_json(df)

        print(f"DataFrame ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {df.memory_usage(deep=True).sum()} bytes")

        # ë°ì´í„° í¬ê¸° í™•ì¸
        if len(df) > 10000:  # ì„ê³„ê°’ ì„¤ì •
            # ë””ë ‰í† ë¦¬ ìƒì„±
            temp_dir = "/opt/airflow/temp_data"
            os.makedirs(temp_dir, exist_ok=True)
            
            # ëŒ€ìš©ëŸ‰ ë°ì´í„°ëŠ” íŒŒì¼ë¡œ ì €ì¥
            file_path = f"{temp_dir}/preprocessed_{context['ds']}.parquet"
            df.to_parquet(file_path)
            context['ti'].xcom_push(key='data_path', value=file_path)
            print(f"ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥: {file_path}")
        else:
            # ì†Œìš©ëŸ‰ ë°ì´í„°ëŠ” XCom ì‚¬ìš©
            # ì•ˆì „í•œ JSON ë³€í™˜
            json_data = safe_dataframe_to_json(df)

            json_size = len(json_data.encode('utf-8'))
            print(f"ì „ì²˜ë¦¬ ì™„ë£Œ. ë°ì´í„° í¬ê¸°: {json_size} bytes")
            
            # XCom í¬ê¸° ì œí•œ í™•ì¸
            if json_size > 1048576:
                temp_dir = "/opt/airflow/temp_data"
                os.makedirs(temp_dir, exist_ok=True)
                file_path = f"{temp_dir}/preprocessed_{context['ds']}.parquet"
                df.to_parquet(file_path)
                context['ti'].xcom_push(key='data_path', value=file_path)
                print(f"XCom í¬ê¸° ì´ˆê³¼ë¡œ íŒŒì¼ ì €ì¥: {file_path}")
            else:
                context['ti'].xcom_push(key='preprocessed_df', value=json_data)
                print("ì „ì²˜ë¦¬ ë°ì´í„° XCom í‘¸ì‹œ ì™„ë£Œ")

    except Exception as e:
        print(f"ì „ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        raise

def modeling_data(**context):
    try:
        print("ëª¨ë¸ë§ ì‹œì‘...")
        
        # íŒŒì¼ ê²½ë¡œ í™•ì¸
        data_path = context['ti'].xcom_pull(key='data_path', task_ids='preprocess_data')
        
        if data_path:
            # íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ
            df = pd.read_parquet(data_path)
            print(f"íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ: {data_path}")
        else:
            # XComì—ì„œ ë°ì´í„° ë¡œë“œ
            json_data = context['ti'].xcom_pull(key='preprocessed_df', task_ids='preprocess_data')
            if not json_data:
                raise ValueError("ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            df = pd.read_json(json_data)
            print("XComì—ì„œ ë°ì´í„° ë¡œë“œ")

        if df.empty:
            raise ValueError("ì „ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
        
        print(f"ëª¨ë¸ë§ ì…ë ¥ ë°ì´í„° í¬ê¸°: {len(df)} rows")
        result_df = model_train(df)
        
        if result_df is None or result_df.empty:
            raise ValueError("ëª¨ë¸ë§ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
        
        print(f"ëª¨ë¸ë§ ì™„ë£Œ. ê²°ê³¼ ë°ì´í„° í¬ê¸°: {len(result_df)} rows")
        
        # **ê²°ê³¼ ë°ì´í„°ë„ ì¸ì½”ë”© ì •ë¦¬**
        result_df = sanitize_dataframe_for_json(result_df)
        
        # ê²°ê³¼ ë°ì´í„° ì €ì¥
        if len(result_df) > 10000:
            # ëŒ€ìš©ëŸ‰ ë°ì´í„°ëŠ” íŒŒì¼ë¡œ ì €ì¥
            temp_dir = "/opt/airflow/temp_data"
            os.makedirs(temp_dir, exist_ok=True)
            file_path = f"{temp_dir}/modeling_result_{context['ds']}.parquet"
            result_df.to_parquet(file_path)
            context['ti'].xcom_push(key='result_path', value=file_path)
            print(f"ëŒ€ìš©ëŸ‰ ëª¨ë¸ë§ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥: {file_path}")
        else:
            # **ì•ˆì „í•œ JSON ë³€í™˜ ì‚¬ìš©**
            result_json = safe_dataframe_to_json(result_df)
            context['ti'].xcom_push(key='modeling_result', value=result_json)
            print("ëª¨ë¸ë§ ê²°ê³¼ XCom í‘¸ì‹œ ì™„ë£Œ")
        
        return "modeling_completed"
        
    except Exception as e:
        print(f"ëª¨ë¸ë§ ì‹¤íŒ¨: {str(e)}")
        raise


def update_data(**context):
    try:
        print("ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸ ì‹œì‘...")
        
        # íŒŒì¼ ê²½ë¡œ í™•ì¸
        result_path = context['ti'].xcom_pull(key='result_path', task_ids='modeling_data')
        
        if result_path:
            # íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ
            df = pd.read_parquet(result_path)
            print(f"íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ: {result_path}")
        else:
            # XComì—ì„œ ë°ì´í„° ë¡œë“œ
            json_data = context['ti'].xcom_pull(key='modeling_result', task_ids='modeling_data')
            if not json_data:
                raise ValueError("ëª¨ë¸ë§ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            df = pd.read_json(json_data)
            print("XComì—ì„œ ë°ì´í„° ë¡œë“œ")

        if df is None or df.empty:
            raise ValueError("ì—…ë°ì´íŠ¸í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
 
        print(f"ì—…ë°ì´íŠ¸í•  ë°ì´í„° í¬ê¸°: {len(df)} rows")
        update_mongo_data(df)
        print("ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
        
    except Exception as e:
        print(f"ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}")
        raise

def cleanup_temp_files(**context):
    """ì„ì‹œ íŒŒì¼ ì •ë¦¬"""
    try:
        temp_dir = "/opt/airflow/temp_data"
        if os.path.exists(temp_dir):
            files_deleted = 0
            for file in os.listdir(temp_dir):
                if context['ds'] in file:  # í•´ë‹¹ ë‚ ì§œì˜ íŒŒì¼ë§Œ ì‚­ì œ
                    file_path = os.path.join(temp_dir, file)
                    os.remove(file_path)
                    print(f"ì„ì‹œ íŒŒì¼ ì‚­ì œ: {file}")
                    files_deleted += 1
            
            if files_deleted == 0:
                print("ì‚­ì œí•  ì„ì‹œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            else:
                print(f"ì´ {files_deleted}ê°œì˜ ì„ì‹œ íŒŒì¼ì„ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")
        else:
            print("ì„ì‹œ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        print(f"ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        # ì •ë¦¬ ì‘ì—… ì‹¤íŒ¨ëŠ” ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì¤‘ë‹¨ì‹œí‚¤ì§€ ì•ŠìŒ

# Task ì •ì˜

# MLflow ì„œë²„ ì‹œì‘
start_mlflow_task = PythonOperator(
    task_id='start_mlflow_server',
    python_callable=start_mlflow_server,
    dag=dag
)

# ë°ì´í„° ì „ì²˜ë¦¬ Task
preprocess_task = PythonOperator(
    task_id='preprocess_data',
    python_callable=preprocess_data,
    dag=dag
)

# ëª¨ë¸ë§ Task
modeling_task = PythonOperator(
    task_id='modeling_data',
    python_callable=modeling_data,
    dag=dag
)

# ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸ Task
update_task = PythonOperator(
    task_id='update_data',
    python_callable=update_data,
    dag=dag
)

# ì„ì‹œ íŒŒì¼ ì •ë¦¬ Task
cleanup_task = PythonOperator(
    task_id='cleanup_temp_files',
    python_callable=cleanup_temp_files,
    dag=dag,
    trigger_rule='all_done'  # ì„±ê³µ/ì‹¤íŒ¨ ê´€ê³„ì—†ì´ ì‹¤í–‰
)

# Task ì˜ì¡´ì„± ì„¤ì •
start_mlflow_task >> preprocess_task >> modeling_task >> update_task >> cleanup_task
